{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.96725321e-04, -7.49883381e-05],\n",
       "       [-6.51000650e-05, -1.31089175e-04],\n",
       "       [ 1.04409818e-04, -2.80525820e-04],\n",
       "       ...,\n",
       "       [-5.52842839e-05,  2.23624056e-05],\n",
       "       [ 6.73737547e-05, -1.12684386e-04],\n",
       "       [ 7.78900189e-05,  1.68112911e-04]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import eigh,norm\n",
    "import logging\n",
    "\n",
    "def load_mnist_dataset():\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "    train_mask_0 = np.isin(y_train, [0])\n",
    "    train_mask_8 = np.isin(y_train, [8])\n",
    "    x_train_0 = x_train[train_mask_0]\n",
    "    x_train_8 = x_train[train_mask_8]\n",
    "    x_train=np.concatenate((x_train_0[:500,:,:],x_train_8[:500,:,:]))\n",
    "    y_train=np.concatenate((y_train[train_mask_0][:500],y_train[train_mask_8][:500]))\n",
    "    \n",
    "    test_mask_0 = np.isin(y_test, [0])\n",
    "    test_mask_8 = np.isin(y_test, [8])\n",
    "    x_test_0 = x_test[test_mask_0]\n",
    "    x_test_8 = x_test[test_mask_8]\n",
    "    x_test=np.concatenate((x_test_0[:200,:,:],x_test_8[:200,:,:]))\n",
    "    y_test=np.concatenate((y_test[test_mask_0][:200],y_test[test_mask_8][:200]))\n",
    "\n",
    "    x_train = x_train.reshape(x_train.shape[0], -1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], -1)\n",
    "    x_train = x_train.astype('float32') / 255.0\n",
    "    x_test = x_test.astype('float32') / 255.0\n",
    "    return x_train,y_train,x_test,y_test\n",
    "\n",
    "class TSNE_unsupervise_module():\n",
    "  def __init__(self, n_components=2, perplexity=20.0, max_iter=500, learning_rate=10,randomstate=1):\n",
    "    \"\"\"A t-Distributed Stochastic Neighbor Embedding implementation.\n",
    "    Parameters\n",
    "    ----------\n",
    "    max_iter : int, default 200\n",
    "    perplexity : float, default 30.0\n",
    "    n_components : int, default 2\n",
    "    \"\"\"\n",
    "    self.max_iter = max_iter\n",
    "    self.perplexity = perplexity\n",
    "    self.n_components = n_components\n",
    "    self.momentum = 0.9\n",
    "    self.lr = learning_rate\n",
    "    self.seed=randomstate\n",
    "\n",
    "  def fit_tranform(self,X):\n",
    "    self.fit(X)\n",
    "    return self.transform(X)\n",
    "\n",
    "  def fit(self,X):\n",
    "    self.Y = np.random.RandomState(self.seed).normal(0., 0.0001, [X.shape[0], 2])\n",
    "    self.Q, self.distances = self.q_tsne()\n",
    "    self.P=self.p_joint(X)\n",
    "\n",
    "  def transform(self,X):\n",
    "    if self.momentum:\n",
    "        Y_m2 = self.Y.copy()\n",
    "        Y_m1 = self.Y.copy()\n",
    "\n",
    "    for i in range(self.max_iter):\n",
    "\n",
    "        # Get Q and distances (distances only used for t-SNE)\n",
    "        self.Q, self.distances = self.q_tsne()\n",
    "        # Estimate gradients with respect to Y\n",
    "        grads = self.tsne_grad()\n",
    "\n",
    "        # Update Y\n",
    "        self.Y = self.Y - self.lr * grads\n",
    "\n",
    "        if self.momentum:  # Add momentum\n",
    "            self.Y += self.momentum * (Y_m1 - Y_m2)\n",
    "            # Update previous Y's for momentum\n",
    "            Y_m2 = Y_m1.copy()\n",
    "            Y_m1 = self.Y.copy()\n",
    "    return self.Y\n",
    "\n",
    "  def p_joint(self,X):\n",
    "    \"\"\"Given a data matrix X, gives joint probabilities matrix.\n",
    "    # Arguments\n",
    "        X: Input data matrix.\n",
    "    # Returns:\n",
    "        P: Matrix with entries p_ij = joint probabilities.\n",
    "    \"\"\"\n",
    "    def p_conditional_to_joint(P):\n",
    "      \"\"\"Given conditional probabilities matrix P, return\n",
    "      approximation of joint distribution probabilities.\"\"\"\n",
    "      return (P + P.T) / (2. * P.shape[0])\n",
    "    def calc_prob_matrix(distances, sigmas=None, zero_index=None):\n",
    "      \"\"\"Convert a distances matrix to a matrix of probabilities.\"\"\"\n",
    "      if sigmas is not None:\n",
    "          two_sig_sq = 2. * np.square(sigmas.reshape((-1, 1)))\n",
    "          return self.softmax(distances / two_sig_sq, zero_index=zero_index)\n",
    "      else:\n",
    "          return self.softmax(distances, zero_index=zero_index)\n",
    "    # Get the negative euclidian distances matrix for our data\n",
    "    distances = self.neg_squared_euc_dists(X)\n",
    "    # Find optimal sigma for each row of this distances matrix\n",
    "    sigmas = self.find_optimal_sigmas()\n",
    "    # Calculate the probabilities based on these optimal sigmas\n",
    "    p_conditional = calc_prob_matrix(distances, sigmas)\n",
    "    # Go from conditional to joint probabilities matrix\n",
    "    self.P = p_conditional_to_joint(p_conditional)\n",
    "    return self.P\n",
    "  \n",
    "\n",
    "  def find_optimal_sigmas(self):\n",
    "    \"\"\"For each row of distances matrix, find sigma that results\n",
    "    in target perplexity for that role.\"\"\"\n",
    "    def binary_search(eval_fn, target, tol=1e-10, max_iter=10000,\n",
    "                  lower=1e-20, upper=1000.):\n",
    "      \"\"\"Perform a binary search over input values to eval_fn.\n",
    "      # Arguments\n",
    "          eval_fn: Function that we are optimising over.\n",
    "          target: Target value we want the function to output.\n",
    "          tol: Float, once our guess is this close to target, stop.\n",
    "          max_iter: Integer, maximum num. iterations to search for.\n",
    "          lower: Float, lower bound of search range.\n",
    "          upper: Float, upper bound of search range.\n",
    "      # Returns:\n",
    "          Float, best input value to function found during search.\n",
    "      \"\"\"\n",
    "      for i in range(max_iter):\n",
    "          guess = (lower + upper) / 2.\n",
    "          val = eval_fn(guess)\n",
    "          if val > target:\n",
    "              upper = guess\n",
    "          else:\n",
    "              lower = guess\n",
    "          if np.abs(val - target) <= tol:\n",
    "              break\n",
    "      return guess\n",
    "    def calc_perplexity(prob_matrix):\n",
    "      \"\"\"Calculate the perplexity of each row\n",
    "      of a matrix of probabilities.\"\"\"\n",
    "      entropy = -np.sum(prob_matrix * np.log2(prob_matrix), 1)\n",
    "      perplexity = 2 ** entropy\n",
    "      return perplexity\n",
    "\n",
    "    def perplexity(distances, sigmas, zero_index):\n",
    "        \"\"\"Wrapper function for quick calculation of\n",
    "        perplexity over a distance matrix.\"\"\"\n",
    "        def calc_prob_matrix(distances, sigmas=None, zero_index=None):\n",
    "          \"\"\"Convert a distances matrix to a matrix of probabilities.\"\"\"\n",
    "          if sigmas is not None:\n",
    "              two_sig_sq = 2. * np.square(sigmas.reshape((-1, 1)))\n",
    "              return self.softmax(distances / two_sig_sq, zero_index=zero_index)\n",
    "          else:\n",
    "              return self.softmax(distances, zero_index=zero_index)\n",
    "        return calc_perplexity(\n",
    "            calc_prob_matrix(distances, sigmas, zero_index))\n",
    "    sigmas = []\n",
    "    # For each row of the matrix (each point in our dataset)\n",
    "    for i in range(self.distances.shape[0]):\n",
    "        # Make fn that returns perplexity of this row given sigma\n",
    "        eval_fn = lambda sigma: \\\n",
    "            perplexity(self.distances[i:i+1, :], np.array(sigma), i)\n",
    "        # Binary search over sigmas to achieve target perplexity\n",
    "        correct_sigma = binary_search(eval_fn, self.perplexity)\n",
    "        # Append the resulting sigma to our output array\n",
    "        sigmas.append(correct_sigma)\n",
    "    return np.array(sigmas)\n",
    "\n",
    "\n",
    "  def tsne_grad(self):\n",
    "    \"\"\"t-SNE: Estimate the gradient of the cost with respect to Y.\"\"\"\n",
    "    pq_diff = self.P - self.Q  # NxN matrix\n",
    "    pq_expanded = np.expand_dims(pq_diff, 2)  # NxNx1\n",
    "    y_diffs = np.expand_dims(self.Y, 1) - np.expand_dims(self.Y, 0)  # NxNx2\n",
    "    # Expand our distances matrix so can multiply by y_diffs\n",
    "    distances_expanded = np.expand_dims(self.distances, 2)  # NxNx1\n",
    "    # Weight this (NxNx2) by distances matrix (NxNx1)\n",
    "    y_diffs_wt = y_diffs * distances_expanded  # NxNx2\n",
    "    grad = 4. * (pq_expanded * y_diffs_wt).sum(1)  # Nx2\n",
    "    return grad\n",
    "\n",
    "  def neg_squared_euc_dists(self,X):\n",
    "      \"\"\"Compute matrix containing negative squared euclidean\n",
    "      distance for all pairs of points in input matrix X\n",
    "      # Arguments:\n",
    "          X: matrix of size NxD\n",
    "      # Returns:\n",
    "          NxN matrix D, with entry D_ij = negative squared\n",
    "          euclidean distance between rows X_i and X_j\n",
    "      \"\"\"\n",
    "      # Math? See https://stackoverflow.com/questions/37009647\n",
    "      sum_X = np.sum(np.square(X), 1)\n",
    "      D = np.add(np.add(-2 * np.dot(X, X.T), sum_X).T, sum_X)\n",
    "      return -D\n",
    "\n",
    "\n",
    "  def softmax(self,X, diag_zero=True, zero_index=None):\n",
    "      \"\"\"Compute softmax values for each row of matrix X.\"\"\"\n",
    "\n",
    "      # Subtract max for numerical stability\n",
    "      e_x = np.exp(X - np.max(X, axis=1).reshape([-1, 1]))\n",
    "\n",
    "      # We usually want diagonal probailities to be 0.\n",
    "      if zero_index is None:\n",
    "          if diag_zero:\n",
    "              np.fill_diagonal(e_x, 0.)\n",
    "      else:\n",
    "          e_x[:, zero_index] = 0.\n",
    "\n",
    "      # Add a tiny constant for stability of log we take later\n",
    "      e_x = e_x + 1e-8  # numerical stability\n",
    "\n",
    "      return e_x / e_x.sum(axis=1).reshape([-1, 1])\n",
    "\n",
    "  def q_tsne(self):\n",
    "    \"\"\"t-SNE: Given low-dimensional representations Y, compute\n",
    "    matrix of joint probabilities with entries q_ij.\"\"\"\n",
    "    distances = self.neg_squared_euc_dists(self.Y)\n",
    "    inv_distances = np.power(1. - distances, -1)\n",
    "    np.fill_diagonal(inv_distances, 0.)\n",
    "    return inv_distances / np.sum(inv_distances), inv_distances\n",
    "x_train,y_train,x_test,y_test=load_mnist_dataset()\n",
    "tsne=TSNE_unsupervise_module(n_components=2)\n",
    "tsne.fit(x_train)\n",
    "tsne.transform(x_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dimensionality-reduction-lunIhGpr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
